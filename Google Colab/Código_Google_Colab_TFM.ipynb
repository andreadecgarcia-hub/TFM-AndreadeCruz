{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip -q install -r colab/Requirements.txt"
      ],
      "metadata": {
        "id": "AwB0T7B02zpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "import os\n",
        "import getpass\n",
        "import re\n",
        "import time\n",
        "import unicodedata\n",
        "from smolagents import ToolCallingAgent, OpenAIServerModel, tool\n",
        "\n",
        "# Configuración\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") or getpass.getpass(\"Introduce OPENAI_API_KEY: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "MODEL_ID = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    return OpenAIServerModel(model_id=MODEL_ID, api_key=OPENAI_API_KEY)\n",
        "\n",
        "#Memoria y utilidades de texto\n",
        "\n",
        "class EvidenceStore:\n",
        "    _data = {}\n",
        "\n",
        "    @classmethod\n",
        "    def reset(cls):\n",
        "        cls._data = {}\n",
        "\n",
        "    @classmethod\n",
        "    def add(cls, key: str, value: str):\n",
        "        cls._data[key] = value\n",
        "\n",
        "    @classmethod\n",
        "    def get(cls):\n",
        "        return dict(cls._data)\n",
        "\n",
        "def _norm(s: str) -> str:\n",
        "    s = s.lower().strip()\n",
        "    s = \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\")\n",
        "    return s\n",
        "\n",
        "    # Validar formato de salida en formato aviso\n",
        "def check_veredicto_completo(final_answer: str):\n",
        "    if not isinstance(final_answer, str):\n",
        "        raise ValueError(\"La salida final no es texto.\")\n",
        "    if not re.search(r\"(?im)^\\s*veredicto\\s*:\\s*(.+)$\", final_answer):\n",
        "        raise ValueError(\"Falta 'Veredicto: ...'.\")\n",
        "    if not re.search(r\"(?im)^\\s*justificaci[oó]n\\s+breve\\s*:\\s*(.+)$\", final_answer):\n",
        "        raise ValueError(\"Falta 'Justificación breve: ...'.\")\n",
        "    if not re.search(r\"(?im)^\\s*confiabilidad\\s*:\\s*(0(\\.\\d+)?|1(\\.0+)?)\\s*$\", final_answer):\n",
        "        raise ValueError(\"Falta 'Confiabilidad: 0.0–1.0'.\")\n",
        "    return True\n",
        "\n",
        "def _extract_veredicto(s: str):\n",
        "    pat_es = r\"(?im)^[\\s\\*\\-_>]*veredicto\\s*:\\s*(verdadero|falso|dudoso)\\b\"\n",
        "    m = list(re.finditer(pat_es, s))\n",
        "    if m:\n",
        "        return m[-1].group(1).lower()\n",
        "\n",
        "    pat_en = r\"(?im)^[\\s\\*\\-_>]*(verdict|decision)\\s*:\\s*(true|false|uncertain|inconclusive)\\b\"\n",
        "    m = list(re.finditer(pat_en, s))\n",
        "    if m:\n",
        "        word = m[-1].group(2).lower()\n",
        "        return {\"true\":\"verdadero\",\"false\":\"falso\",\"uncertain\":\"dudoso\",\"inconclusive\":\"dudoso\"}[word]\n",
        "\n",
        "    return None\n",
        "\n",
        "def _extract_confiabilidad(s: str):\n",
        "    num = r\"(0(?:[.,]\\d+)?|1(?:[.,]0+)?)\"\n",
        "    m = re.findall(rf\"(?im)^\\s*confiabilidad\\s*:\\s*{num}\\s*$\", s)\n",
        "    if m:\n",
        "        return m[-1].replace(\",\", \".\")\n",
        "\n",
        "    m = re.findall(rf\"(?im)\\bconfiabilidad\\s*:\\s*{num}\\b\", s)\n",
        "    if m:\n",
        "        return m[-1].replace(\",\", \".\")\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def _extract_justificacion(s: str):\n",
        "    num = r\"(0(?:[.,]\\d+)?|1(?:[.,]0+)?)\"\n",
        "    pat = rf\"(?im)^[\\s\\*\\-_>]*justificaci[oó]n\\s+breve\\s*:\\s*(.+?)(?:\\s+confiabilidad\\s*:\\s*{num}\\s*)?$\"\n",
        "    m = re.search(pat, s)\n",
        "    return m.group(1).strip() if m else None\n",
        "\n",
        "#Prompts fijos\n",
        "\n",
        "SYSTEM_PROMPT_JURADO = (\n",
        "    \"Eres un jurado de IA. Tu única tarea es decidir si una afirmación es \"\n",
        "    \"VERDADERA, FALSA o DUDOSA basándote en los análisis aportados.\\n\\n\"\n",
        "    \"Devuelve SIEMPRE en el siguiente formato EXACTO (en español):\\n\\n\"\n",
        "    \"Veredicto: <Verdadero|Falso|Dudoso>\\n\"\n",
        "    \"Justificación breve: <máx. 3 frases, concretas>\\n\"\n",
        "    \"Confiabilidad: <número entre 0.0 y 1.0>\\n\"\n",
        ")\n",
        "\n",
        "SUB_PROMPT_SENSACIONALISMO = (\n",
        "    \"Analiza si la afirmación usa lenguaje sensacionalista/emocional. \"\n",
        "    \"Devuelve 2-4 frases, objetivas y concisas.\"\n",
        ")\n",
        "SUB_PROMPT_GRAMATICA = (\n",
        "    \"Revisa la afirmación y detecta errores gramaticales/ortográficos/estilo. \"\n",
        "    \"Devuelve 2-4 frases, claras y útiles que indiquen si hay errores gramaticales/ortográficos/estilo o por lo contrario si esta correctamente escrita la afirmación.\"\n",
        ")\n",
        "SUB_PROMPT_SENTIDO_COMUN = (\n",
        "    \"Evalúa si la afirmación contradice el sentido común. \"\n",
        "    \"Devuelve 2-4 frases, con razonamiento breve.\"\n",
        ")\n",
        "\n",
        "# Crear Agentes\n",
        "def make_agent(instructions: str, tools=None, max_steps: int = 6) -> ToolCallingAgent:\n",
        "    tools = tools or []\n",
        "    try:\n",
        "        return ToolCallingAgent(\n",
        "            model=build_model(),\n",
        "            tools=tools,\n",
        "            instructions=instructions,\n",
        "            max_steps=max_steps,\n",
        "        )\n",
        "    except TypeError:\n",
        "        try:\n",
        "            return ToolCallingAgent(\n",
        "                model=build_model(),\n",
        "                tools=tools,\n",
        "                instructions=instructions,\n",
        "            )\n",
        "        except TypeError:\n",
        "            agent = ToolCallingAgent(model=build_model(), tools=tools)\n",
        "            if hasattr(agent, \"instructions\"):\n",
        "                agent.instructions = instructions\n",
        "            elif hasattr(agent, \"system_prompt\"):\n",
        "                agent.system_prompt = instructions\n",
        "            return agent\n",
        "\n",
        "def run_with_retry(agent: ToolCallingAgent, prompt: str, retries: int = 2, backoff: float = 1.0) -> str:\n",
        "    for i in range(retries + 1):\n",
        "        try:\n",
        "            return agent.run(prompt)\n",
        "        except Exception:\n",
        "            if i == retries:\n",
        "                raise\n",
        "            time.sleep(backoff * (i + 1))\n",
        "\n",
        "#Subagentes/Tools\n",
        "\n",
        "@tool\n",
        "def evaluar_sentido_comun(texto: str) -> str:\n",
        "    \"\"\"Evalúa si la afirmación contradice el sentido común.\n",
        "    Args:\n",
        "        texto (str): La afirmación a evaluar.\n",
        "    Returns:\n",
        "        str: Análisis breve (2–4 frases).\n",
        "    \"\"\"\n",
        "    out = subagente_texto(SUB_PROMPT_SENTIDO_COMUN, texto)\n",
        "    EvidenceStore.add(\"sentido_comun\", out)\n",
        "    return out\n",
        "\n",
        "@tool\n",
        "def evaluar_sensacionalismo(texto: str) -> str:\n",
        "    \"\"\"Detecta uso de lenguaje sensacionalista o emocional.\n",
        "    Args:\n",
        "        texto (str): La afirmación a evaluar.\n",
        "    Returns:\n",
        "        str: Análisis breve (2–4 frases).\n",
        "    \"\"\"\n",
        "    out = subagente_texto(SUB_PROMPT_SENSACIONALISMO, texto)\n",
        "    EvidenceStore.add(\"sensacionalismo\", out)\n",
        "    return out\n",
        "\n",
        "@tool\n",
        "def evaluar_gramatica(texto: str) -> str:\n",
        "    \"\"\"Revisa gramática, ortografía y estilo.\n",
        "    Args:\n",
        "        texto (str): La afirmación a evaluar.\n",
        "    Returns:\n",
        "        str: Análisis breve (2–4 frases).\n",
        "    \"\"\"\n",
        "    out = subagente_texto(SUB_PROMPT_GRAMATICA, texto)\n",
        "    EvidenceStore.add(\"gramatica\", out)\n",
        "    return out\n",
        "\n",
        "def subagente_texto(instrucciones: str, afirmacion: str) -> str:\n",
        "    agent = make_agent(instrucciones, tools=[])\n",
        "    return run_with_retry(agent, f\"Afirmación: {afirmacion}\", retries=1, backoff=0.8)\n",
        "\n",
        "# Analizar afirmación\n",
        "def analizar_afirmacion(claim: str) -> str:\n",
        "    claim = str(claim).strip()\n",
        "    if not claim:\n",
        "        raise ValueError(\"La afirmación está vacía.\")\n",
        "\n",
        "    EvidenceStore.reset()\n",
        "\n",
        "    jurado = make_agent(\n",
        "        SYSTEM_PROMPT_JURADO + (\n",
        "            \"\\n\\nDEBES recopilar evidencia llamando a herramientas antes del veredicto. \"\n",
        "            \"Llama a (1) evaluar_sensacionalismo, (2) evaluar_gramatica y \"\n",
        "            \"(3) evaluar_sentido_comun sobre la MISMA afirmación, y solo después dicta veredicto.\"\n",
        "        ),\n",
        "        tools=[evaluar_sensacionalismo, evaluar_gramatica, evaluar_sentido_comun],\n",
        "        max_steps=12,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        jurado.verbose = False\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    prompt_veredicto = (\n",
        "        f'Afirmación: \"{claim}\"\\n\\n'\n",
        "        \"Primero, usa las herramientas indicadas con el texto de la afirmación. \"\n",
        "        \"Cuando tengas suficiente evidencia, responde únicamente en este formato EXACTO:\\n\\n\"\n",
        "        \"Veredicto: <Verdadero|Falso|Dudoso>\\n\"\n",
        "        \"Justificación breve: <máx. 3 frases, concretas>\\n\"\n",
        "        \"Confiabilidad: <número entre 0.0 y 1.0>\\n\"\n",
        "        \"No incluyas 'Confiabilidad' dentro de 'Justificación breve'.\"\n",
        "    )\n",
        "\n",
        "\n",
        "    salida = run_with_retry(jurado, prompt_veredicto, retries=2, backoff=0.8)\n",
        "\n",
        "    aviso_validator = \"\"\n",
        "    try:\n",
        "        check_veredicto_completo(salida)\n",
        "    except Exception as e:\n",
        "        aviso_validator = f\"\\n\\n> **Aviso del validador**: {e}\"\n",
        "\n",
        "    veredicto_final     = _extract_veredicto(salida) or \"—\"\n",
        "    justificacion_final = _extract_justificacion(salida) or \"—\"\n",
        "    confianza_final     = _extract_confiabilidad(salida) or \"—\"\n",
        "\n",
        "\n",
        "    evid = EvidenceStore.get()\n",
        "    evid_sens = evid.get(\"sensacionalismo\", \"— (no se invocó la tool)\")\n",
        "    evid_gram = evid.get(\"gramatica\", \"— (no se invocó la tool)\")\n",
        "    evid_comn = evid.get(\"sentido_comun\", \"— (no se invocó la tool)\")\n",
        "\n",
        "    # Markdown final\n",
        "    resultado_md = (\n",
        "        f\"**Veredicto:** {veredicto_final}\\n\\n\"\n",
        "        f\"**Justificación breve:** {justificacion_final}\\n\\n\"\n",
        "        f\"**Confiabilidad:** {confianza_final}{aviso_validator}\\n\\n\"\n",
        "        \"---\\n\\n\"\n",
        "        f\"### Evidencias de subagentes\\n\"\n",
        "        f\"- **Sensacionalismo:** {evid_sens}\\n\"\n",
        "        f\"- **Gramática:** {evid_gram}\\n\"\n",
        "        f\"- **Sentido común:** {evid_comn}\\n\"\n",
        "    )\n",
        "\n",
        "    return resultado_md"
      ],
      "metadata": {
        "id": "Hr-ag6XxNm-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultado = analizar_afirmacion(\"El agujero en la capa de ozono sobre la Antártida se detectó por primera vez en los años 80.\")\n",
        "print(resultado)\n"
      ],
      "metadata": {
        "id": "IuNI0i1WPE7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "7P7GMTV4PrC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "import re, time\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "#Evaluación y rendimiento\n",
        "try:\n",
        "    ENC = tiktoken.get_encoding(\"o200k_base\")\n",
        "except Exception:\n",
        "    ENC = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(ENC.encode(text or \"\"))\n",
        "\n",
        "MAPEO = {\n",
        "    \"verdadero\":\"Verdadero\",\"true\":\"Verdadero\",\"real\":\"Verdadero\",\"cierto\":\"Verdadero\",\n",
        "    \"correcto\":\"Verdadero\",\"veraz\":\"Verdadero\",\"no es falso\":\"Verdadero\",\"no es fake\":\"Verdadero\",\n",
        "    \"no fake\":\"Verdadero\",\"not false\":\"Verdadero\",\"not fake\":\"Verdadero\",\n",
        "    \"falso\":\"Falso\",\"false\":\"Falso\",\"fake\":\"Falso\",\"fake news\":\"Falso\",\"engaño\":\"Falso\",\n",
        "    \"hoax\":\"Falso\",\"incorrecto\":\"Falso\",\n",
        "    \"dudoso\":\"Dudoso\",\"dudosa\":\"Dudoso\",\"incierto\":\"Dudoso\",\"inconcluso\":\"Dudoso\",\n",
        "    \"uncertain\":\"Dudoso\",\"inconclusive\":\"Dudoso\",\"half-true\":\"Dudoso\",\"rumor\":\"Dudoso\"\n",
        "}\n",
        "def mapeo(x: str) -> str:\n",
        "    s = str(x).strip().lower()\n",
        "    return MAPEO.get(s, s.title())\n",
        "\n",
        "\n",
        "def extraer_md_veredicto_vfd(md_text: str) -> str:\n",
        "    if not isinstance(md_text, str):\n",
        "        return \"Dudoso\"\n",
        "    pat = r\"(?im)^[\\s>*-]*\\**\\s*veredicto\\s*:\\s*\\**\\s*([a-záéíóúüñ]+)\"\n",
        "    m = re.search(pat, md_text or \"\")\n",
        "    if not m:\n",
        "        return \"Dudoso\"\n",
        "    v = m.group(1).strip().lower().rstrip(\".\")\n",
        "    if v.startswith(\"verdader\"): return \"Verdadero\"\n",
        "    if v.startswith(\"falso\") or v.startswith(\"fake\"): return \"Falso\"\n",
        "    if v.startswith(\"dudoso\") or v.startswith(\"dudosa\"): return \"Dudoso\"\n",
        "    return \"Dudoso\"\n",
        "\n",
        "def extraer_md_confianza(md_text: str) -> str:\n",
        "    num = r\"(0(?:[.,]\\d+)?|1(?:[.,]0+)?)\"\n",
        "    m = re.search(rf\"(?im)^[\\s>*-]*\\**\\s*confiabilidad\\s*:\\s*\\**\\s*{num}\\b\", md_text or \"\")\n",
        "    return m.group(1).replace(\",\", \".\") if m else \"—\"\n",
        "\n",
        "EXCEL_PATH = \"/colab/data/dataset_pruebas.xlsx\"\n",
        "OUTPUT_CSV = \"/colab/data/resultados_con_evidencias.csv\"\n",
        "\n",
        "df_eval = pd.read_excel(EXCEL_PATH)\n",
        "assert \"Afirmación\" in df_eval.columns, \"No encuentro la columna 'Afirmación'.\"\n",
        "assert \"Categoria\" in df_eval.columns, \"No encuentro la columna 'Categoria'.\"\n",
        "df_eval[\"y_true\"] = df_eval[\"Categoria\"].apply(mapeo)\n",
        "\n",
        "preds, confs = [], []\n",
        "times, in_toks, out_toks, tot_toks = [], [], [], []\n",
        "\n",
        "for i, texto in enumerate(df_eval[\"Afirmación\"], start=1):\n",
        "    texto = str(texto)\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    md = analizar_afirmacion(texto)\n",
        "    elapsed = time.perf_counter() - t0\n",
        "    times.append(round(elapsed, 3))\n",
        "\n",
        "\n",
        "    preds.append(extraer_md_veredicto_vfd(md))\n",
        "    confs.append(extraer_md_confianza(md))\n",
        "\n",
        "    in_est = count_tokens(f'Afirmación: \"{texto}\"')\n",
        "    try:\n",
        "        in_est += count_tokens(SYSTEM_PROMPT_JURADO)\n",
        "        in_est += count_tokens(SUB_PROMPT_SENSACIONALISMO + f\"Afirmación: {texto}\")\n",
        "        in_est += count_tokens(SUB_PROMPT_GRAMATICA + f\"Afirmación: {texto}\")\n",
        "        in_est += count_tokens(SUB_PROMPT_SENTIDO_COMUN + f\"Afirmación: {texto}\")\n",
        "    except NameError:\n",
        "        pass\n",
        "    out_est = count_tokens(md)\n",
        "\n",
        "    in_toks.append(in_est)\n",
        "    out_toks.append(out_est)\n",
        "    tot_toks.append(in_est + out_est)\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Procesadas {i} filas...\")\n",
        "\n",
        "# Guardar resultados mínimos\n",
        "df_eval[\"y_pred\"] = preds\n",
        "df_eval[\"confiabilidad\"] = confs\n",
        "df_eval[\"tiempo_s\"] = times\n",
        "df_eval[\"input_tokens_est\"] = in_toks\n",
        "df_eval[\"output_tokens_est\"] = out_toks\n",
        "df_eval[\"total_tokens_est\"] = tot_toks\n",
        "\n",
        "df_eval.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
        "print(f\"Resultados guardados en: {OUTPUT_CSV}\")\n",
        "\n",
        "#Métricas básicas\n",
        "labels_vfd = [\"Falso\", \"Dudoso\", \"Verdadero\"]\n",
        "print(\"\\n=== Accuracy ===\", accuracy_score(df_eval[\"y_true\"], df_eval[\"y_pred\"]))\n",
        "print(\"\\n=== Classification report ===\")\n",
        "print(classification_report(df_eval[\"y_true\"], df_eval[\"y_pred\"],\n",
        "                            labels=labels_vfd, target_names=labels_vfd, zero_division=0))\n",
        "\n",
        "# 5)Tiempo y tokens por afirmación\n",
        "ids = df_eval.index + 1\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.bar(ids, df_eval[\"tiempo_s\"])\n",
        "plt.title(\"Duración total por afirmación (s)\")\n",
        "plt.xlabel(\"ID de afirmación\")\n",
        "plt.ylabel(\"Tiempo (segundos)\")\n",
        "plt.xticks(ids)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.bar(ids, df_eval[\"total_tokens_est\"])\n",
        "plt.title(\"Tokens procesados por afirmación (total estimado)\")\n",
        "plt.xlabel(\"ID de afirmación\")\n",
        "plt.ylabel(\"Tokens\")\n",
        "plt.xticks(ids)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "IHVpU15YCrZh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}